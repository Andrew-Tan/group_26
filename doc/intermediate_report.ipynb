{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting IMDB Ratings Based on Movie Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authors: Yuanzhe Marco Ma, Arash Shamseddini, Kaicheng Tan, Zhenrui Yu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Project Goal](#0)\n",
    "- [Data Retrieval](#1)\n",
    "- [Exploratory Data Analysis (EDA)](#2)\n",
    "- [Preprocessing](#3)\n",
    "- [ML Model Fitting](#4)\n",
    "- [Predicting](#5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we look at the relationship between movie's reviews and their IMDB scores (ranging from 0 ~ 5 stars). Positive reviews are often related to high IMDB scores, while negative reviews indicate the opposite. While it is easy for humans to understand a piece of review and guess the scores, we wonder if machines could understand it as well. Furthermore, we would like to automate this process, so that given a bunch of movie reviews, we are able to predict their corresponding IMDB scores easily. <br>\n",
    "\n",
    "In this project, we attempt to use Support Vector Machines as our machine learning model, aided by other statistical analysis tools such as ___ and ___. **[TODO]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where Do We Get the Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained our data from an open-sourced github repository: <br> \n",
    "> https://github.com/nproellochs/SentimentDictionaries/blob/master/Dataset_IMDB.csv <br>\n",
    "\n",
    "The repository was originally used for sentiment analysis related to movie reviews. Here we are using the `Dataset_IMDB.csv` as our main data source. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To automate our analysis, we have written a script to retrieve the dataset with Python. The script can be accessed [here](\"https://github.com/UBC-MDS/group_26/blob/main/src/download_data.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does the data look like?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into the dataset by performing some Exploratory Data Analysis (EDA). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Columns:\n",
    "| Column Name | Column Type | Description                             |\n",
    "|-------------|-------------|-----------------------------------------|\n",
    "| Id          | Numeric     | Unique ID assigned to each observation. |\n",
    "| Text        | Free Text   | Body of the review content.             |\n",
    "| Author      | Categorical | Author's name of the review             |\n",
    "| Rating      | Numeric     | Ratings given along with the review     |\n",
    "\n",
    "For this project, we look primarily into the `Text` and `Rating` columns. <br> Therefore, we will **drop** the `Author` and `Id` columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. The `Text` feature\n",
    "The `Text` feature will be our primarily input feature. Here is a distribution of the lengths of the reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also looked into the top 10 most frequent words in the reviews. They are shown below: \n",
    "\n",
    "| Word   | Frequency | Rank |\n",
    "|--------|-----------|------|\n",
    "|the     |   172557  |  1   |\n",
    "|of      |   78038   |  2   |\n",
    "|and     |   76392   |  3   |\n",
    "|to      |   74239   |  4   |\n",
    "|is      |   57547   |  5   |\n",
    "|in      |   49646   |  6   |\n",
    "|that    |   33476   |  7   |\n",
    "|it      |   33061   |  8   |\n",
    "|as      |   27536   |  9   |\n",
    "|with    |   26852   |  10  |\n",
    "\n",
    "As we can see, the most frequent words are often generic words such as prepositions and pronouns, which has little implication to our model learning. We might want to avoid overfitting to these words as we train our model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. The `Rating` Class\n",
    "`Ratings` will be our target class. Let's look at a distribution of `Rating`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating distribution plot here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratings seem roughly normally distributed, with a little skewness to the left. Most of the ratings cluster around 0.5 ~ 0.8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Correlation between `Text` length and `Rating`\n",
    "We suspect that people more passionate about certain movies tend to write longer reviews to express feelings. This could also be true for very negative reviews. <br> <br>\n",
    "A bar plot of `Text` length vs `Rating` is presented below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review length vs rating plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't seem to be a strong correlation between reviews length and rating. However, it is notable that for the most positive ratings (at 0.9-1.0), the reviews are the highest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to `Text` feature alone, we extracted two potentially useful columns that could enhance our learning model. \n",
    "\n",
    "#### 1. `length`\n",
    "As mentioned above, we suspect some correlation between review lengths and ratings. Therefore a feature of `Text` lengths is created. \n",
    "#### 2. `sentiment`\n",
    "We utilized the [**NLTK**](\"https://www.nltk.org/\") package to assist us in extracting the sentiment of each review. This `sentiment` feature will have four ordinal categories - ['neg', 'compound', 'neu', 'pos']. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our training data ready, we will fit our regression model using a Support Vector Machine. <br>\n",
    "Here we utilized `sklearn`'s SVM estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tuned our model with hyper-parameter optimization. Specifically, we tuned the `Gamma` hyperparameter of SVM using [GridSearchCV]('website'). The `Gamma` hyperparameter controls the complexity of our model. We want to pick the best `Gamma` so that our model does a decent job in predicting while avoiding over-fitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The table below shows our tuned, optimal model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some model table here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting with Our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finalizing our model, we tested it on our test set. \n",
    "\n",
    "#### 1. $R^2$ Score\n",
    "\n",
    "$R^2$ is a common metric for evaluating a regression model's accuracy. <br> \n",
    "The obtained $R^2$ score for the test set is ____, which was _____ to our cross validation score. Our model performed _____."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Prediction vs. True Ratings\n",
    "We have also created a scatterplot to compare our predicted ratings vs. the true ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction vs. true value plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is an obvious difference in patterns between the predicted ratings and true ratings. In the true ratings, people tend to give whole number rating, i.e. 0.3 instead of 0.3247. Our model did not capture that. <br>\n",
    "\n",
    "Despite that, most of the points are clustered around the identity line. This indicates that our model didn't seem to under-fit or over-fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticism and Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are done with our prediction and analysis, we can examine the quality of our work. \n",
    "In fact, there are a few areas of improvement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We discarded the `Author` and `id` columns at the beginning. In fact, these columns may be useful features especially `Author`, because certain authors may have a certain type of audiences, who tend to wrtie certain type of reviews. This could provide information helpful to our learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The `sentiment` feature genereated with the NLTK package contained only `neu` and `compound` in our dataset. This is confusing and we have yet to understand this behavior. We included the feature regardless because it may still provide useful information. However, this is definitely a place we need to investigate further into. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As shown on the predicition vs true rating plot above, some of our predictions went beyond the rating limit of 10. We could have set some sort of boundary for our prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As mentioned above, our model did not capture the pattern where humans tend to give whole number scores. In the future, we can probably give more emphasis to predicting whole number scores, so that our model resembles more human behaviors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
